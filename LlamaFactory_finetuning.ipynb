{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 环境配置"
      ],
      "metadata": {
        "id": "6mSDDM9upGpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_G_EI9KNDw5-",
        "outputId": "71db1eee-8d04-4931-f9ac-56bc9365d05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr  1 08:25:59 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uname -m && cat /etc/*release"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLD2_9FVJHrS",
        "outputId": "437ea8c8-9acc-45a4-96bf-5728259ec9eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x86_64\n",
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuaG2PohQhaO",
        "outputId": "67b03401-5640-4be8-ad0b-ee29f1535f59"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps xformers==0.0.25\n",
        "!pip install .[bitsandbytes]\n",
        "!pip install -e \".[torch,metrics,bitsandbytes]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xl_yTWoGdKF",
        "outputId": "af32b4ee-9a17-474c-8520-ed88ad600406"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 346, done.\u001b[K\n",
            "remote: Counting objects: 100% (346/346), done.\u001b[K\n",
            "remote: Compressing objects: 100% (285/285), done.\u001b[K\n",
            "remote: Total 346 (delta 80), reused 168 (delta 45), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (346/346), 9.54 MiB | 19.41 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mdocker\u001b[0m/      LICENSE      pyproject.toml  requirements.txt  \u001b[01;34msrc\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mevaluation\u001b[0m/  Makefile     README.md       \u001b[01;34mscripts\u001b[0m/          \u001b[01;34mtests\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         \u001b[01;34mexamples\u001b[0m/    MANIFEST.in  README_zh.md    setup.py\n",
            "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
            "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-5p_mv1_4/unsloth_e58b50382dc142c2bde31bd53694639b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-5p_mv1_4/unsloth_e58b50382dc142c2bde31bd53694639b\n",
            "  Resolved https://github.com/unslothai/unsloth.git to commit b44f89e60ac7ec54fdc18a58e8f0905c4d6a79f2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: unsloth_zoo>=2025.3.17 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.3.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
            "Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
            "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.50.0)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
            "Requirement already satisfied: protobuf<4.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.29.3)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.9)\n",
            "Requirement already satisfied: bitsandbytes>=0.43.3 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.4)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.2)\n",
            "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.9.6)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n",
            "Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.1.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n",
            "Requirement already satisfied: xformers==0.0.25 in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Processing /content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.50.0)\n",
            "Requirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
            "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.21.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.2.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.45.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.14)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (2.27.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building wheel for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-py3-none-any.whl size=287960 sha256=9794f1ed959b751cdc1d4c18bf866c8e62d29f90ecee1b555fd8c75ae67b7e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.3.dev0\n",
            "    Uninstalling llamafactory-0.9.3.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
            "Successfully installed llamafactory-0.9.3.dev0\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.50.0)\n",
            "Requirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
            "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.21.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.2.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
            "Requirement already satisfied: rouge-chinese in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.0.3)\n",
            "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.45.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.29.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.14)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (2.27.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (2.5.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26099 sha256=32101c8f581c6b377dd0bb7020ff54c0d3a39429853316db1c280f0f85592437\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-618e5s3y/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.3.dev0\n",
            "    Uninstalling llamafactory-0.9.3.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
            "Successfully installed llamafactory-0.9.3.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果出现依赖错误，请运行一下命令行"
      ],
      "metadata": {
        "id": "X9ao36A0O_D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDrOhcJHXWV",
        "outputId": "5c237161-6b17-4e4f-bbb5-9d6737595aa5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: llamafactory\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26073 sha256=08952c8f5170b37b3a67d8dc8280218e0c8bfa16cf69db9688a3c5dfa3ac48c7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4_zaaom7/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "Successfully built llamafactory\n",
            "Installing collected packages: llamafactory\n",
            "  Attempting uninstall: llamafactory\n",
            "    Found existing installation: llamafactory 0.9.3.dev0\n",
            "    Uninstalling llamafactory-0.9.3.dev0:\n",
            "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
            "Successfully installed llamafactory-0.9.3.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!llamafactory-cli version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlkvCGW8Ij4K",
        "outputId": "18ea48ed-d9fb-4c9f-b16f-4408cbc7bfa7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-01 08:26:53.766254: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743496013.789070   18442 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743496013.795363   18442 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-01 08:26:53.820415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "----------------------------------------------------------\n",
            "| Welcome to LLaMA Factory, version 0.9.3.dev0           |\n",
            "|                                                        |\n",
            "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
            "----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 修改认知数据集"
      ],
      "metadata": {
        "id": "4NThyMTHpAKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"超级无敌鸿蒙机器人\"\n",
        "AUTHOR = \"Sun Guyi\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a84PEVnKg2hR",
        "outputId": "08f557c6-3f3f-476f-dd53-8b57c52797e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **添加微调的训练集，并将其注册到数据集中**\n",
        "\n"
      ],
      "metadata": {
        "id": "hnO_lyEgdk6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 进行微调"
      ],
      "metadata": {
        "id": "7Rjm7Y5Vo647"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "args = dict(\n",
        "  stage=\"sft\",                                               # 进行指令监督微调\n",
        "  do_train=True,\n",
        "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
        "  dataset=\"identity,data0\",          # 使用 alpaca 和自我认知数据集\n",
        "  template=\"llama3\",                                         # 使用 llama3 提示词模板\n",
        "  finetuning_type=\"lora\",                                    # 使用 LoRA 适配器来节省显存\n",
        "  lora_target=\"all\",                                         # 添加 LoRA 适配器至全部线性层\n",
        "  output_dir=\"llama3_lora\",                                  # 保存 LoRA 适配器的路径\n",
        "  per_device_train_batch_size=2,                             # 批处理大小\n",
        "  gradient_accumulation_steps=4,                             # 梯度累积步数\n",
        "  lr_scheduler_type=\"cosine\",                                # 使用余弦学习率退火算法\n",
        "  logging_steps=5,                                           # 每 5 步输出一个记录\n",
        "  warmup_ratio=0.1,                                          # 使用预热学习率\n",
        "  save_steps=1000,                                           # 每 1000 步保存一个检查点\n",
        "  learning_rate=5e-5,                                        # 学习率大小\n",
        "  num_train_epochs=3.0,                                      # 训练轮数\n",
        "  max_samples=300,                                           # 使用每个数据集中的 300 条样本\n",
        "  max_grad_norm=1.0,                                         # 将梯度范数裁剪至 1.0\n",
        "  loraplus_lr_ratio=16.0,                                    # 使用 LoRA+ 算法并设置 lambda=16.0\n",
        "  fp16=True,                                                 # 使用 float16 混合精度训练\n",
        "  report_to=\"none\",                                          # 关闭 wandb 记录器\n",
        ")\n",
        "\n",
        "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "!llamafactory-cli train train_llama3.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ8MEcM9g_9c",
        "outputId": "3b487314-3e6a-4ea9-a816-41c1a6813203"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-04-01 08:28:59.294566: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743496139.316459   18984 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743496139.322927   18984 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-01 08:28:59.345136: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO|2025-04-01 08:29:07] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:08,202 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 08:29:08,722 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 08:29:09,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 08:29:09,671 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,903 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,904 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,904 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,904 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,904 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 08:29:09,904 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 08:29:10,390 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-01 08:29:10] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
            "[WARNING|2025-04-01 08:29:10] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|2025-04-01 08:29:10] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
            "Generating train split: 91 examples [00:00, 10268.26 examples/s]\n",
            "Converting format of dataset: 100% 91/91 [00:00<00:00, 8671.43 examples/s]\n",
            "[INFO|2025-04-01 08:29:11] llamafactory.data.loader:143 >> Loading dataset dataset0.json...\n",
            "Generating train split: 1053 examples [00:00, 39315.65 examples/s]\n",
            "Converting format of dataset: 100% 300/300 [00:00<00:00, 12510.97 examples/s]\n",
            "Running tokenizer on dataset: 100% 391/391 [00:00<00:00, 1120.35 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 113617, 53434, 43292, 116522, 106101, 123, 110919, 33748, 32648, 17792, 11, 459, 15592, 18328, 8040, 555, 8219, 26340, 72, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "inputs:\n",
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello! I am 超级无敌鸿蒙机器人, an AI assistant developed by Sun Guyi. How can I assist you today?<|eot_id|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 113617, 53434, 43292, 116522, 106101, 123, 110919, 33748, 32648, 17792, 11, 459, 15592, 18328, 8040, 555, 8219, 26340, 72, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
            "labels:\n",
            "Hello! I am 超级无敌鸿蒙机器人, an AI assistant developed by Sun Guyi. How can I assist you today?<|eot_id|>\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 08:29:12,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 08:29:12,050 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 08:29:12] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|quantization_config.py:436] 2025-04-01 08:29:12,091 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-01 08:29:12,151 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2170] 2025-04-01 08:29:12,154 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 08:29:12,156 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4987] 2025-04-01 08:29:36,394 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-01 08:29:36,395 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-01 08:29:36,627 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 08:29:36,627 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 08:29:36] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-01 08:29:36] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-01 08:29:36] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-01 08:29:36] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-04-01 08:29:36] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,k_proj,v_proj,gate_proj,up_proj,o_proj,down_proj\n",
            "[INFO|2025-04-01 08:29:37] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "[INFO|trainer.py:748] 2025-04-01 08:29:37,154 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:783] 2025-04-01 08:29:37,154 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|2025-04-01 08:29:37] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2409] 2025-04-01 08:29:37,617 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-04-01 08:29:37,617 >>   Num examples = 391\n",
            "[INFO|trainer.py:2411] 2025-04-01 08:29:37,617 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-04-01 08:29:37,618 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-04-01 08:29:37,618 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-04-01 08:29:37,618 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-04-01 08:29:37,618 >>   Total optimization steps = 147\n",
            "[INFO|trainer.py:2418] 2025-04-01 08:29:37,622 >>   Number of trainable parameters = 20,971,520\n",
            "{'loss': 1.0733, 'grad_norm': 1.398769736289978, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.1}\n",
            "{'loss': 0.9255, 'grad_norm': 0.6176431179046631, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5973, 'grad_norm': 0.36805909872055054, 'learning_rate': 5e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4006, 'grad_norm': 0.5923447608947754, 'learning_rate': 4.982319711683221e-05, 'epoch': 0.41}\n",
            "{'loss': 0.6186, 'grad_norm': 0.49072930216789246, 'learning_rate': 4.929528920808854e-05, 'epoch': 0.51}\n",
            "{'loss': 0.5507, 'grad_norm': 0.6095996499061584, 'learning_rate': 4.842374312499405e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4093, 'grad_norm': 0.687667727470398, 'learning_rate': 4.722088621637309e-05, 'epoch': 0.71}\n",
            "{'loss': 0.5275, 'grad_norm': 0.9173218011856079, 'learning_rate': 4.570373196778427e-05, 'epoch': 0.82}\n",
            "{'loss': 0.3869, 'grad_norm': 0.640651524066925, 'learning_rate': 4.389373935885646e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4292, 'grad_norm': 1.029510498046875, 'learning_rate': 4.181650934253132e-05, 'epoch': 1.02}\n",
            "{'loss': 0.3105, 'grad_norm': 1.0067473649978638, 'learning_rate': 3.9501422739279956e-05, 'epoch': 1.12}\n",
            "{'loss': 0.305, 'grad_norm': 0.3670862019062042, 'learning_rate': 3.6981224668001424e-05, 'epoch': 1.22}\n",
            "{'loss': 0.2591, 'grad_norm': 0.7174178957939148, 'learning_rate': 3.4291561391508185e-05, 'epoch': 1.33}\n",
            "{'loss': 0.1474, 'grad_norm': 0.24165058135986328, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.43}\n",
            "{'loss': 0.2536, 'grad_norm': 0.7001808285713196, 'learning_rate': 2.8557870956832132e-05, 'epoch': 1.53}\n",
            "{'loss': 0.1811, 'grad_norm': 0.6842416524887085, 'learning_rate': 2.5594942438652688e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3125, 'grad_norm': 0.5373212695121765, 'learning_rate': 2.2623598917395438e-05, 'epoch': 1.73}\n",
            "{'loss': 0.2458, 'grad_norm': 0.6811863780021667, 'learning_rate': 1.9685867761175584e-05, 'epoch': 1.84}\n",
            "{'loss': 0.2137, 'grad_norm': 0.4321129024028778, 'learning_rate': 1.682330091706446e-05, 'epoch': 1.94}\n",
            "{'loss': 0.1887, 'grad_norm': 0.676371693611145, 'learning_rate': 1.4076387190766017e-05, 'epoch': 2.04}\n",
            "{'loss': 0.0988, 'grad_norm': 0.17689545452594757, 'learning_rate': 1.148397956361007e-05, 'epoch': 2.14}\n",
            "{'loss': 0.1355, 'grad_norm': 0.4568327069282532, 'learning_rate': 9.082745647022797e-06, 'epoch': 2.24}\n",
            "{'loss': 0.1764, 'grad_norm': 0.5948092341423035, 'learning_rate': 6.906649047373246e-06, 'epoch': 2.35}\n",
            "{'loss': 0.1161, 'grad_norm': 0.4590866267681122, 'learning_rate': 4.986468976890993e-06, 'epoch': 2.45}\n",
            "{'loss': 0.1325, 'grad_norm': 0.5991172790527344, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.55}\n",
            "{'loss': 0.1278, 'grad_norm': 0.3805334270000458, 'learning_rate': 2.0184924104583613e-06, 'epoch': 2.65}\n",
            "{'loss': 0.1273, 'grad_norm': 0.45694729685783386, 'learning_rate': 1.0126756596375686e-06, 'epoch': 2.76}\n",
            "{'loss': 0.1009, 'grad_norm': 0.1931418478488922, 'learning_rate': 3.4614115704533767e-07, 'epoch': 2.86}\n",
            "{'loss': 0.1213, 'grad_norm': 0.3893616199493408, 'learning_rate': 2.831652042480093e-08, 'epoch': 2.96}\n",
            "100% 147/147 [31:57<00:00, 12.45s/it][INFO|trainer.py:3966] 2025-04-01 09:01:35,218 >> Saving model checkpoint to llama3_lora/checkpoint-147\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:01:35,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:01:35,671 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-01 09:01:35,882 >> tokenizer config file saved in llama3_lora/checkpoint-147/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-01 09:01:35,882 >> Special tokens file saved in llama3_lora/checkpoint-147/special_tokens_map.json\n",
            "[INFO|trainer.py:2665] 2025-04-01 09:01:41,727 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1924.106, 'train_samples_per_second': 0.61, 'train_steps_per_second': 0.076, 'train_loss': 0.3234007464987891, 'epoch': 3.0}\n",
            "100% 147/147 [32:04<00:00, 13.09s/it]\n",
            "[INFO|trainer.py:3966] 2025-04-01 09:01:41,729 >> Saving model checkpoint to llama3_lora\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:01:42,200 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:01:42,201 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-01 09:01:42,410 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-01 09:01:42,410 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 17036670GF\n",
            "  train_loss               =     0.3234\n",
            "  train_runtime            = 0:32:04.10\n",
            "  train_samples_per_second =       0.61\n",
            "  train_steps_per_second   =      0.076\n",
            "[INFO|modelcard.py:449] 2025-04-01 09:01:42,681 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用webUI进行对话"
      ],
      "metadata": {
        "id": "M8h9zQacpMin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFaPFRsgf7NZ",
        "outputId": "2925212f-f73c-4ab6-eec2-6e42ce373809"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-04-01 09:02:08.534064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743498128.556253   27347 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743498128.562930   27347 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-01 09:02:08.588696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "* Running on public URL: https://c00fb5309b96324759.gradio.live\n",
            "[WARNING|2025-04-01 09:06:00] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:00,717 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 09:06:01,209 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:06:02,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:06:02,058 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:06:02,254 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 09:06:02,709 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-01 09:06:02] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
            "[WARNING|2025-04-01 09:06:02] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:06:02,933 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:06:02,934 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 09:06:02] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2025-04-01 09:06:02] llamafactory.model.patcher:143 >> Using KV cache for faster generation.\n",
            "[INFO|quantization_config.py:436] 2025-04-01 09:06:02,989 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-01 09:06:03,053 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2170] 2025-04-01 09:06:03,056 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 09:06:03,058 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255\n",
            "}\n",
            "\n",
            "[INFO|quantizer_bnb_4bit.py:124] 2025-04-01 09:06:03,217 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "[INFO|modeling_utils.py:4987] 2025-04-01 09:06:13,978 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-01 09:06:13,979 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-01 09:06:14,171 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 09:06:14,172 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 09:06:14] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-01 09:06:15] llamafactory.model.adapter:143 >> Loaded adapter(s): /content/LLaMA-Factory/llama3_lora/checkpoint-147\n",
            "[INFO|2025-04-01 09:06:15] llamafactory.model.loader:143 >> all params: 8,051,232,768\n",
            "[WARNING|2025-04-01 09:06:15] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n",
            "[WARNING|2025-04-01 09:12:27] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,888 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,889 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,889 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,889 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,889 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:27,889 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 09:12:28,382 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:12:29,310 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:12:29,310 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file tokenizer.model from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-01 09:12:29,546 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-01 09:12:30,007 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-01 09:12:30] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
            "[WARNING|2025-04-01 09:12:30] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
            "[INFO|configuration_utils.py:699] 2025-04-01 09:12:30,329 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-01 09:12:30,330 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": null,\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"unsloth_version\": \"2024.9\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 09:12:30] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
            "[INFO|2025-04-01 09:12:30] llamafactory.model.patcher:143 >> Using KV cache for faster generation.\n",
            "[INFO|quantization_config.py:436] 2025-04-01 09:12:30,332 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-01 09:12:30,334 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
            "[INFO|modeling_utils.py:2170] 2025-04-01 09:12:30,338 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 09:12:30,339 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128009,\n",
            "  \"pad_token_id\": 128255\n",
            "}\n",
            "\n",
            "[INFO|quantizer_bnb_4bit.py:124] 2025-04-01 09:12:30,488 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
            "[INFO|modeling_utils.py:4987] 2025-04-01 09:12:33,941 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-01 09:12:33,942 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-01 09:12:34,173 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-01 09:12:34,173 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 8192,\n",
            "  \"pad_token_id\": 128255,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "[INFO|2025-04-01 09:12:34] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-01 09:12:35] llamafactory.model.adapter:143 >> Loaded adapter(s): /content/LLaMA-Factory/llama3_lora\n",
            "[INFO|2025-04-01 09:12:35] llamafactory.model.loader:143 >> all params: 8,051,232,768\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://c00fb5309b96324759.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}