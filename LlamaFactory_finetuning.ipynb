{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 环境配置"
   ],
   "metadata": {
    "id": "6mSDDM9upGpF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G_EI9KNDw5-",
    "outputId": "d03d0be5-0dca-4881-de35-6be4738389dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sat Apr  5 07:21:48 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!uname -m && cat /etc/*release"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLD2_9FVJHrS",
    "outputId": "a2195652-e80a-4b5a-af70-c5a7709d3185"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x86_64\n",
      "DISTRIB_ID=Ubuntu\n",
      "DISTRIB_RELEASE=22.04\n",
      "DISTRIB_CODENAME=jammy\n",
      "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n",
      "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"22.04\"\n",
      "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
      "VERSION_CODENAME=jammy\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=jammy\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!gcc --version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuaG2PohQhaO",
    "outputId": "799cb9a6-f7e0-48a3-8a57-e7dcd95930d0"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "Copyright (C) 2021 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/\n",
    "%rm -rf LLaMA-Factory\n",
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers==0.0.25\n",
    "!pip install .[bitsandbytes]\n",
    "!pip install -e \".[torch,metrics,bitsandbytes]\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-xl_yTWoGdKF",
    "outputId": "44929d0a-3966-4eac-e684-fcd04f4014ff"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n",
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 348, done.\u001B[K\n",
      "remote: Counting objects: 100% (348/348), done.\u001B[K\n",
      "remote: Compressing objects: 100% (289/289), done.\u001B[K\n",
      "remote: Total 348 (delta 83), reused 154 (delta 44), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (348/348), 9.60 MiB | 8.92 MiB/s, done.\n",
      "Resolving deltas: 100% (83/83), done.\n",
      "/content/LLaMA-Factory\n",
      "\u001B[0m\u001B[01;34massets\u001B[0m/       \u001B[01;34mdocker\u001B[0m/      LICENSE      pyproject.toml  requirements.txt  \u001B[01;34msrc\u001B[0m/\n",
      "CITATION.cff  \u001B[01;34mevaluation\u001B[0m/  Makefile     README.md       \u001B[01;34mscripts\u001B[0m/          \u001B[01;34mtests\u001B[0m/\n",
      "\u001B[01;34mdata\u001B[0m/         \u001B[01;34mexamples\u001B[0m/    MANIFEST.in  README_zh.md    setup.py\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-2vmj6cw2/unsloth_c95945a8f1364a689cde5182124b2206\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-2vmj6cw2/unsloth_c95945a8f1364a689cde5182124b2206\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit bb112e38ef3f0dafa9e87faf55a6ba7499bd0357\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting unsloth_zoo>=2025.3.17 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading unsloth_zoo-2025.3.17-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2)\n",
      "Collecting tyro (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading tyro-0.9.18-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.50.3)\n",
      "Collecting datasets>=2.16.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (5.9.5)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.45.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.0.2)\n",
      "Collecting protobuf<4.0.0 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.30.1)\n",
      "Collecting hf_transfer (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting bitsandbytes>=0.43.3 (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Collecting xxhash (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.11.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.5.3)\n",
      "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.2.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.2)\n",
      "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.14.0)\n",
      "Collecting cut_cross_entropy (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.3.17->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.1.0)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.9.4)\n",
      "Collecting shtab>=1.5.6 (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2025.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes>=0.43.3->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.0/76.0 MB\u001B[0m \u001B[31m10.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m491.2/491.2 kB\u001B[0m \u001B[31m36.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m162.1/162.1 kB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading unsloth_zoo-2025.3.17-py3-none-any.whl (127 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.8/127.8 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m98.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tyro-0.9.18-py3-none-any.whl (123 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m123.6/123.6 kB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m12.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m183.9/183.9 kB\u001B[0m \u001B[31m18.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m143.5/143.5 kB\u001B[0m \u001B[31m15.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/13.8 MB\u001B[0m \u001B[31m107.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.6/24.6 MB\u001B[0m \u001B[31m79.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m883.7/883.7 kB\u001B[0m \u001B[31m52.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m95.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m318.9/318.9 kB\u001B[0m \u001B[31m28.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.8/194.8 kB\u001B[0m \u001B[31m18.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for unsloth: filename=unsloth-2025.3.19-py3-none-any.whl size=192274 sha256=ae17d9ea4915946b4276bb69a803d639503352817f8c7765985197eda778e237\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-agl1fw30/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: xxhash, unsloth, shtab, protobuf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, datasets, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed bitsandbytes-0.45.4 cut_cross_entropy-25.1.1 datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 hf_transfer-0.1.9 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 protobuf-3.20.3 shtab-1.7.1 trl-0.15.2 tyro-0.9.18 unsloth-2025.3.19 unsloth_zoo-2025.3.17 xxhash-3.5.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "google"
        ]
       },
       "id": "69f0f0d30c7d414192077ca549f0245f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting xformers==0.0.25\n",
      "  Downloading xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Downloading xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl (222.6 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m222.6/222.6 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xformers\n",
      "Successfully installed xformers-0.0.25\n",
      "Processing /content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting datasets<=3.4.1,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<=0.21.0,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
      "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.11.1)\n",
      "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
      "Collecting fire (from llamafactory==0.9.3.dev0)\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m87.2/87.2 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Collecting numpy<2.0.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.0/61.0 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pydantic (from llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.45.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Collecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Collecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
      "Collecting pydub (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m487.4/487.4 kB\u001B[0m \u001B[31m35.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m46.2/46.2 MB\u001B[0m \u001B[31m13.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m322.1/322.1 kB\u001B[0m \u001B[31m26.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m95.2/95.2 kB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m101.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m431.7/431.7 kB\u001B[0m \u001B[31m33.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m78.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m101.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.2/10.2 MB\u001B[0m \u001B[31m48.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m245.8/245.8 kB\u001B[0m \u001B[31m20.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m109.8/109.8 kB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.3/62.3 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.7/39.7 MB\u001B[0m \u001B[31m17.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.2/1.2 MB\u001B[0m \u001B[31m57.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.3/11.3 MB\u001B[0m \u001B[31m125.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m72.0/72.0 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: llamafactory, fire\n",
      "  Building wheel for llamafactory (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-py3-none-any.whl size=288695 sha256=8401d41c9d402faba353575f22507e70186eb0530df2a33be2cb87dc926baa39\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
      "  Building wheel for fire (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=708597e80a7639913ce36d46ff3231db03603e4544d8bf515ab9e89030e61adc\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
      "Successfully built llamafactory fire\n",
      "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, numpy, markupsafe, groovy, fire, ffmpy, av, aiofiles, tiktoken, starlette, pydantic, tyro, tokenizers, sse-starlette, safehttpx, gradio-client, fastapi, transformers, gradio, datasets, trl, llamafactory\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.33.0\n",
      "    Uninstalling pydantic_core-2.33.0:\n",
      "      Successfully uninstalled pydantic_core-2.33.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: markupsafe\n",
      "    Found existing installation: MarkupSafe 3.0.2\n",
      "    Uninstalling MarkupSafe-3.0.2:\n",
      "      Successfully uninstalled MarkupSafe-3.0.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.11.1\n",
      "    Uninstalling pydantic-2.11.1:\n",
      "      Successfully uninstalled pydantic-2.11.1\n",
      "  Attempting uninstall: tyro\n",
      "    Found existing installation: tyro 0.9.18\n",
      "    Uninstalling tyro-0.9.18:\n",
      "      Successfully uninstalled tyro-0.9.18\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.50.3\n",
      "    Uninstalling transformers-4.50.3:\n",
      "      Successfully uninstalled transformers-4.50.3\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.5.0\n",
      "    Uninstalling datasets-3.5.0:\n",
      "      Successfully uninstalled datasets-3.5.0\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.15.2\n",
      "    Uninstalling trl-0.15.2:\n",
      "      Successfully uninstalled trl-0.15.2\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.25 requires torch==2.2.1, but you have torch 2.6.0+cu124 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed aiofiles-23.2.1 av-14.2.0 datasets-3.4.1 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 numpy-1.26.4 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 sse-starlette-2.2.1 starlette-0.46.1 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 transformers-4.50.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0\n",
      "Obtaining file:///content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.50.0)\n",
      "Requirement already satisfied: datasets<=3.4.1,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.4.1)\n",
      "Requirement already satisfied: accelerate<=1.5.2,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: peft<=0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
      "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.21.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.20.3)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.2.0)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n",
      "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.42.1)\n",
      "Collecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: bitsandbytes>=0.39.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.45.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.5.2,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.7.2)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.1)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (2.27.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.50.0,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.1)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (6.3.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.4.1,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26070 sha256=c2badd449f27c29863559aff183adb532edfa20437f77e00566857c2292b3a15\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wn2la_ux/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: rouge-chinese, llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.3.dev0\n",
      "    Uninstalling llamafactory-0.9.3.dev0:\n",
      "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
      "Successfully installed llamafactory-0.9.3.dev0 rouge-chinese-1.0.3\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果出现依赖错误，请运行一下命令行"
   ],
   "metadata": {
    "id": "X9ao36A0O_D7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --no-deps -e ."
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSDrOhcJHXWV",
    "outputId": "ec633f15-27e4-4777-d420-60669a4d73d9"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Obtaining file:///content/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build editable ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26070 sha256=324a1e0efd6565df7b405673f1005fb942f845166e46bde86731c63b0a823eda\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8jk5wlt1/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.3.dev0\n",
      "    Uninstalling llamafactory-0.9.3.dev0:\n",
      "      Successfully uninstalled llamafactory-0.9.3.dev0\n",
      "Successfully installed llamafactory-0.9.3.dev0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!llamafactory-cli version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlkvCGW8Ij4K",
    "outputId": "3fb3927a-0723-482f-c7fc-f29be8d3f8f1"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2025-04-05 07:26:09.869009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743837970.103423    1570 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743837970.184034    1570 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 07:26:10.789807: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "----------------------------------------------------------\n",
      "| Welcome to LLaMA Factory, version 0.9.3.dev0           |\n",
      "|                                                        |\n",
      "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
      "----------------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 修改认知数据集"
   ],
   "metadata": {
    "id": "4NThyMTHpAKT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "NAME = \"Android-Harmony Translation Bot\"\n",
    "AUTHOR = \"Sun Guyi\"\n",
    "\n",
    "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
    "\n",
    "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a84PEVnKg2hR",
    "outputId": "d631e6e7-2410-49de-e08e-c5431f06d8b8"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/LLaMA-Factory\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **添加微调的训练集，并将其注册到数据集中**\n",
    "\n"
   ],
   "metadata": {
    "id": "hnO_lyEgdk6f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 进行微调"
   ],
   "metadata": {
    "id": "7Rjm7Y5Vo647"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",                                               # 进行指令监督微调\n",
    "  do_train=True,\n",
    "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # 使用 4 比特量化版 Llama-3-8b-Instruct 模型\n",
    "  dataset=\"identity,data0,data1,data2\",          # 使用 alpaca 和自我认知数据集\n",
    "  template=\"llama3\",                                         # 使用 llama3 提示词模板\n",
    "  finetuning_type=\"lora\",                                    # 使用 LoRA 适配器来节省显存\n",
    "  lora_target=\"all\",                                         # 添加 LoRA 适配器至全部线性层\n",
    "  output_dir=\"llama3_lora\",                                  # 保存 LoRA 适配器的路径\n",
    "  per_device_train_batch_size=2,                             # 批处理大小\n",
    "  gradient_accumulation_steps=4,                             # 梯度累积步数\n",
    "  lr_scheduler_type=\"cosine\",                                # 使用余弦学习率退火算法\n",
    "  logging_steps=5,                                           # 每 5 步输出一个记录\n",
    "  warmup_ratio=0.1,                                          # 使用预热学习率\n",
    "  save_steps=1000,                                           # 每 1000 步保存一个检查点\n",
    "  learning_rate=5e-5,                                        # 学习率大小\n",
    "  num_train_epochs=3.0,                                      # 训练轮数\n",
    "  max_samples=1000,                                           # 使用每个数据集中的 800 条样本\n",
    "  max_grad_norm=1.0,                                         # 将梯度范数裁剪至 1.0\n",
    "  loraplus_lr_ratio=16.0,                                    # 使用 LoRA+ 算法并设置 lambda=16.0\n",
    "  fp16=True,                                                 # 使用 float16 混合精度训练\n",
    "  report_to=\"none\",                                          # 关闭 wandb 记录器\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "!llamafactory-cli train train_llama3.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJ8MEcM9g_9c",
    "outputId": "e57056c8-70c6-4713-d5b3-758921f25de4"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/LLaMA-Factory\n",
      "2025-04-05 07:28:47.328268: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743838127.349399    2274 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743838127.355585    2274 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 07:28:47.377107: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[INFO|2025-04-05 07:28:54] llamafactory.hparams.parser:383 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
      "tokenizer_config.json: 100% 51.1k/51.1k [00:00<00:00, 116MB/s]\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:01<00:00, 7.98MB/s]\n",
      "special_tokens_map.json: 100% 345/345 [00:00<00:00, 1.57MB/s]\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:28:58,490 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-05 07:28:58,963 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "config.json: 100% 1.26k/1.26k [00:00<00:00, 9.05MB/s]\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 07:29:00,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 07:29:00,325 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 07:29:00,631 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-05 07:29:01,094 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-04-05 07:29:01] llamafactory.data.template:143 >> Add <|eot_id|>,<|eom_id|> to stop words.\n",
      "[WARNING|2025-04-05 07:29:01] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|2025-04-05 07:29:01] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
      "Generating train split: 91 examples [00:00, 1781.94 examples/s]\n",
      "Converting format of dataset: 100% 91/91 [00:00<00:00, 8047.26 examples/s]\n",
      "[INFO|2025-04-05 07:29:01] llamafactory.data.loader:143 >> Loading dataset dataset0.json...\n",
      "Generating train split: 1053 examples [00:00, 42378.90 examples/s]\n",
      "Converting format of dataset: 100% 300/300 [00:00<00:00, 11061.32 examples/s]\n",
      "[INFO|2025-04-05 07:29:02] llamafactory.data.loader:143 >> Loading dataset dataset1.json...\n",
      "Generating train split: 1303 examples [00:00, 36281.17 examples/s]\n",
      "Converting format of dataset: 100% 300/300 [00:00<00:00, 11176.56 examples/s]\n",
      "[INFO|2025-04-05 07:29:02] llamafactory.data.loader:143 >> Loading dataset dataset2.json...\n",
      "Generating train split: 313 examples [00:00, 23083.72 examples/s]\n",
      "Converting format of dataset: 100% 300/300 [00:00<00:00, 10983.59 examples/s]\n",
      "Running tokenizer on dataset: 100% 991/991 [00:01<00:00, 819.16 examples/s]\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 6151, 128009, 128006, 78191, 128007, 271, 9906, 0, 358, 1097, 8682, 11529, 277, 65527, 39141, 23869, 11, 459, 15592, 18328, 8040, 555, 8219, 26340, 72, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hi<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hello! I am Android-Harmony Translation Bot, an AI assistant developed by Sun Guyi. How can I assist you today?<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 9906, 0, 358, 1097, 8682, 11529, 277, 65527, 39141, 23869, 11, 459, 15592, 18328, 8040, 555, 8219, 26340, 72, 13, 2650, 649, 358, 7945, 499, 3432, 30, 128009]\n",
      "labels:\n",
      "Hello! I am Android-Harmony Translation Bot, an AI assistant developed by Sun Guyi. How can I assist you today?<|eot_id|>\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 07:29:04,383 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 07:29:04,384 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-04-05 07:29:04] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[INFO|2025-04-05 07:29:04] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|quantization_config.py:436] 2025-04-05 07:29:04,686 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "model.safetensors: 100% 5.70G/5.70G [00:36<00:00, 156MB/s]\n",
      "[INFO|modeling_utils.py:1154] 2025-04-05 07:29:42,442 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
      "[INFO|modeling_utils.py:2170] 2025-04-05 07:29:42,526 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1139] 2025-04-05 07:29:42,529 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4987] 2025-04-05 07:30:08,455 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4995] 2025-04-05 07:30:08,456 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "generation_config.json: 100% 220/220 [00:00<00:00, 1.69MB/s]\n",
      "[INFO|configuration_utils.py:1094] 2025-04-05 07:30:09,403 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
      "[INFO|configuration_utils.py:1139] 2025-04-05 07:30:09,404 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 8192,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,o_proj,k_proj,down_proj,gate_proj,q_proj,v_proj\n",
      "[INFO|2025-04-05 07:30:09] llamafactory.model.loader:143 >> trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "[INFO|trainer.py:748] 2025-04-05 07:30:09,925 >> Using auto half precision backend\n",
      "[WARNING|trainer.py:783] 2025-04-05 07:30:09,926 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO|2025-04-05 07:30:10] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2409] 2025-04-05 07:30:10,402 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-04-05 07:30:10,402 >>   Num examples = 991\n",
      "[INFO|trainer.py:2411] 2025-04-05 07:30:10,402 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-04-05 07:30:10,402 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2415] 2025-04-05 07:30:10,402 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2416] 2025-04-05 07:30:10,402 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2417] 2025-04-05 07:30:10,402 >>   Total optimization steps = 372\n",
      "[INFO|trainer.py:2418] 2025-04-05 07:30:10,406 >>   Number of trainable parameters = 20,971,520\n",
      "{'loss': 0.9717, 'grad_norm': 0.7734725475311279, 'learning_rate': 6.578947368421053e-06, 'epoch': 0.04}\n",
      "{'loss': 1.0005, 'grad_norm': 0.4679051637649536, 'learning_rate': 1.3157894736842106e-05, 'epoch': 0.08}\n",
      "{'loss': 0.801, 'grad_norm': 0.579322338104248, 'learning_rate': 1.9736842105263158e-05, 'epoch': 0.12}\n",
      "{'loss': 0.8198, 'grad_norm': 0.5519304275512695, 'learning_rate': 2.6315789473684212e-05, 'epoch': 0.16}\n",
      "{'loss': 0.5937, 'grad_norm': 0.3645170032978058, 'learning_rate': 3.289473684210527e-05, 'epoch': 0.2}\n",
      "{'loss': 0.6583, 'grad_norm': 0.4500812292098999, 'learning_rate': 3.9473684210526316e-05, 'epoch': 0.24}\n",
      "{'loss': 0.5516, 'grad_norm': 0.8614538908004761, 'learning_rate': 4.605263157894737e-05, 'epoch': 0.28}\n",
      "{'loss': 0.5329, 'grad_norm': 0.45345401763916016, 'learning_rate': 4.999557652060729e-05, 'epoch': 0.32}\n",
      "{'loss': 0.4998, 'grad_norm': 0.4110972285270691, 'learning_rate': 4.994583035302723e-05, 'epoch': 0.36}\n",
      "{'loss': 0.5479, 'grad_norm': 0.41229379177093506, 'learning_rate': 4.984091904405793e-05, 'epoch': 0.4}\n",
      "{'loss': 0.4548, 'grad_norm': 0.4292580187320709, 'learning_rate': 4.968107459428265e-05, 'epoch': 0.44}\n",
      "{'loss': 0.3938, 'grad_norm': 0.33522099256515503, 'learning_rate': 4.946665048328287e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4572, 'grad_norm': 0.5778763890266418, 'learning_rate': 4.91981208879544e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4036, 'grad_norm': 0.3551924526691437, 'learning_rate': 4.887607963391394e-05, 'epoch': 0.56}\n",
      "{'loss': 0.5269, 'grad_norm': 1.0528393983840942, 'learning_rate': 4.8501238882314715e-05, 'epoch': 0.6}\n",
      "{'loss': 0.3636, 'grad_norm': 0.6210731267929077, 'learning_rate': 4.807442755497524e-05, 'epoch': 0.65}\n",
      "{'loss': 0.5252, 'grad_norm': 0.49247825145721436, 'learning_rate': 4.759658950130385e-05, 'epoch': 0.69}\n",
      "{'loss': 0.4387, 'grad_norm': 0.43448761105537415, 'learning_rate': 4.7068781411072686e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3912, 'grad_norm': 0.5535138249397278, 'learning_rate': 4.649217047765685e-05, 'epoch': 0.77}\n",
      "{'loss': 0.4344, 'grad_norm': 0.4230194091796875, 'learning_rate': 4.586803181690609e-05, 'epoch': 0.81}\n",
      "{'loss': 0.4735, 'grad_norm': 0.46678638458251953, 'learning_rate': 4.519774564735711e-05, 'epoch': 0.85}\n",
      "{'loss': 0.5859, 'grad_norm': 0.4146054983139038, 'learning_rate': 4.448279423802207e-05, 'epoch': 0.89}\n",
      "{'loss': 0.4447, 'grad_norm': 0.6688017845153809, 'learning_rate': 4.372475863050286e-05, 'epoch': 0.93}\n",
      "{'loss': 0.3605, 'grad_norm': 0.577698290348053, 'learning_rate': 4.292531514268008e-05, 'epoch': 0.97}\n",
      "{'loss': 0.5211, 'grad_norm': 1.0422073602676392, 'learning_rate': 4.2086231661708185e-05, 'epoch': 1.01}\n",
      "{'loss': 0.3357, 'grad_norm': 0.5283905267715454, 'learning_rate': 4.1209363734514674e-05, 'epoch': 1.05}\n",
      "{'loss': 0.169, 'grad_norm': 0.3733639419078827, 'learning_rate': 4.0296650464448616e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2333, 'grad_norm': 0.44074133038520813, 'learning_rate': 3.9350110223152844e-05, 'epoch': 1.13}\n",
      "{'loss': 0.3294, 'grad_norm': 0.668364942073822, 'learning_rate': 3.837183618714233e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2359, 'grad_norm': 0.48667609691619873, 'learning_rate': 3.7363991708959386e-05, 'epoch': 1.21}\n",
      "{'loss': 0.189, 'grad_norm': 0.3867628574371338, 'learning_rate': 3.6328805533141684e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2389, 'grad_norm': 0.7462199330329895, 'learning_rate': 3.526856686758269e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2067, 'grad_norm': 0.4589856266975403, 'learning_rate': 3.4185620321183545e-05, 'epoch': 1.33}\n",
      "{'loss': 0.3047, 'grad_norm': 0.7812200784683228, 'learning_rate': 3.3082360718991304e-05, 'epoch': 1.37}\n",
      "{'loss': 0.2165, 'grad_norm': 0.582520604133606, 'learning_rate': 3.196122780628936e-05, 'epoch': 1.41}\n",
      "{'loss': 0.2064, 'grad_norm': 0.7777491807937622, 'learning_rate': 3.082470085335133e-05, 'epoch': 1.45}\n",
      "{'loss': 0.2373, 'grad_norm': 0.5951137542724609, 'learning_rate': 2.9675293172789583e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2076, 'grad_norm': 0.5324241518974304, 'learning_rate': 2.8515546561622462e-05, 'epoch': 1.53}\n",
      "{'loss': 0.1778, 'grad_norm': 0.4322178363800049, 'learning_rate': 2.7348025680351363e-05, 'epoch': 1.57}\n",
      "{'loss': 0.2203, 'grad_norm': 0.6516990065574646, 'learning_rate': 2.6175312381477442e-05, 'epoch': 1.61}\n",
      "{'loss': 0.2263, 'grad_norm': 0.4622845947742462, 'learning_rate': 2.5e-05, 'epoch': 1.65}\n",
      "{'loss': 0.2562, 'grad_norm': 0.5877407789230347, 'learning_rate': 2.3824687618522567e-05, 'epoch': 1.69}\n",
      "{'loss': 0.2467, 'grad_norm': 0.5019245743751526, 'learning_rate': 2.2651974319648643e-05, 'epoch': 1.73}\n",
      "{'loss': 0.2345, 'grad_norm': 0.4635390639305115, 'learning_rate': 2.148445343837755e-05, 'epoch': 1.77}\n",
      "{'loss': 0.185, 'grad_norm': 0.5141911506652832, 'learning_rate': 2.032470682721042e-05, 'epoch': 1.81}\n",
      "{'loss': 0.367, 'grad_norm': 0.6747252941131592, 'learning_rate': 1.9175299146648674e-05, 'epoch': 1.85}\n",
      "{'loss': 0.2145, 'grad_norm': 0.5773912668228149, 'learning_rate': 1.8038772193710646e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2578, 'grad_norm': 0.5221087336540222, 'learning_rate': 1.69176392810087e-05, 'epoch': 1.94}\n",
      "{'loss': 0.2374, 'grad_norm': 0.5723071098327637, 'learning_rate': 1.581437967881647e-05, 'epoch': 1.98}\n",
      "{'loss': 0.1588, 'grad_norm': 0.5492756366729736, 'learning_rate': 1.4731433132417316e-05, 'epoch': 2.02}\n",
      "{'loss': 0.0726, 'grad_norm': 0.3977086842060089, 'learning_rate': 1.3671194466858334e-05, 'epoch': 2.06}\n",
      "{'loss': 0.0741, 'grad_norm': 0.301651269197464, 'learning_rate': 1.2636008291040618e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1355, 'grad_norm': 0.5013838410377502, 'learning_rate': 1.1628163812857674e-05, 'epoch': 2.14}\n",
      "{'loss': 0.0889, 'grad_norm': 0.5724342465400696, 'learning_rate': 1.0649889776847161e-05, 'epoch': 2.18}\n",
      "{'loss': 0.1138, 'grad_norm': 0.44598835706710815, 'learning_rate': 9.703349535551387e-06, 'epoch': 2.22}\n",
      "{'loss': 0.0857, 'grad_norm': 0.32617849111557007, 'learning_rate': 8.790636265485334e-06, 'epoch': 2.26}\n",
      "{'loss': 0.096, 'grad_norm': 0.41038212180137634, 'learning_rate': 7.913768338291821e-06, 'epoch': 2.3}\n",
      "{'loss': 0.1378, 'grad_norm': 0.68011474609375, 'learning_rate': 7.074684857319927e-06, 'epoch': 2.34}\n",
      "{'loss': 0.0864, 'grad_norm': 0.3561331033706665, 'learning_rate': 6.275241369497142e-06, 'epoch': 2.38}\n",
      "{'loss': 0.0806, 'grad_norm': 0.3985004127025604, 'learning_rate': 5.51720576197794e-06, 'epoch': 2.42}\n",
      "{'loss': 0.0863, 'grad_norm': 0.3576716482639313, 'learning_rate': 4.802254352642882e-06, 'epoch': 2.46}\n",
      "{'loss': 0.0788, 'grad_norm': 0.4765699505805969, 'learning_rate': 4.131968183093912e-06, 'epoch': 2.5}\n",
      "{'loss': 0.1288, 'grad_norm': 0.5173424482345581, 'learning_rate': 3.5078295223431536e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0962, 'grad_norm': 0.5127251744270325, 'learning_rate': 2.931218588927315e-06, 'epoch': 2.58}\n",
      "{'loss': 0.0606, 'grad_norm': 0.6211744546890259, 'learning_rate': 2.4034104986961627e-06, 'epoch': 2.62}\n",
      "{'loss': 0.0826, 'grad_norm': 0.6548818349838257, 'learning_rate': 1.9255724450247674e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1171, 'grad_norm': 0.7864281535148621, 'learning_rate': 1.4987611176852878e-06, 'epoch': 2.7}\n",
      "{'loss': 0.0781, 'grad_norm': 0.7071175575256348, 'learning_rate': 1.1239203660860648e-06, 'epoch': 2.74}\n",
      "{'loss': 0.0627, 'grad_norm': 0.5021300315856934, 'learning_rate': 8.018791120456087e-07, 'epoch': 2.78}\n",
      "{'loss': 0.0975, 'grad_norm': 0.38251248002052307, 'learning_rate': 5.333495167171353e-07, 'epoch': 2.82}\n",
      "{'loss': 0.08, 'grad_norm': 0.40416383743286133, 'learning_rate': 3.189254057173491e-07, 'epoch': 2.86}\n",
      "{'loss': 0.0816, 'grad_norm': 0.21174438297748566, 'learning_rate': 1.5908095594207583e-07, 'epoch': 2.9}\n",
      "{'loss': 0.0879, 'grad_norm': 0.4907497763633728, 'learning_rate': 5.416964697276261e-08, 'epoch': 2.94}\n",
      "{'loss': 0.0724, 'grad_norm': 0.41082796454429626, 'learning_rate': 4.423479392709484e-09, 'epoch': 2.98}\n",
      "100% 372/372 [1:44:05<00:00, 18.46s/it][INFO|trainer.py:3966] 2025-04-05 09:14:16,031 >> Saving model checkpoint to llama3_lora/checkpoint-372\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 09:14:17,803 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 09:14:17,804 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-05 09:14:18,001 >> tokenizer config file saved in llama3_lora/checkpoint-372/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-05 09:14:18,001 >> Special tokens file saved in llama3_lora/checkpoint-372/special_tokens_map.json\n",
      "[INFO|trainer.py:2665] 2025-04-05 09:14:18,815 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 6248.4088, 'train_samples_per_second': 0.476, 'train_steps_per_second': 0.06, 'train_loss': 0.2941499142557062, 'epoch': 3.0}\n",
      "100% 372/372 [1:44:08<00:00, 16.80s/it]\n",
      "[INFO|trainer.py:3966] 2025-04-05 09:14:18,817 >> Saving model checkpoint to llama3_lora\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 09:14:19,459 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 09:14:19,460 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-04-05 09:14:19,771 >> tokenizer config file saved in llama3_lora/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-04-05 09:14:19,772 >> Special tokens file saved in llama3_lora/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 67735558GF\n",
      "  train_loss               =     0.2941\n",
      "  train_runtime            = 1:44:08.40\n",
      "  train_samples_per_second =      0.476\n",
      "  train_steps_per_second   =       0.06\n",
      "[INFO|modelcard.py:449] 2025-04-05 09:14:20,076 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 保存微调的结果并下载"
   ],
   "metadata": {
    "id": "kMFgUN3g3wHR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! zip -r llama3_lora.zip llama3_lora"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40HnIBGY310t",
    "outputId": "334c17a4-0158-4163-9025-29726d989589"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: llama3_lora/ (stored 0%)\n",
      "  adding: llama3_lora/trainer_log.jsonl (deflated 80%)\n",
      "  adding: llama3_lora/README.md (deflated 48%)\n",
      "  adding: llama3_lora/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3_lora/trainer_state.json (deflated 77%)\n",
      "  adding: llama3_lora/training_args.bin (deflated 51%)\n",
      "  adding: llama3_lora/adapter_config.json (deflated 55%)\n",
      "  adding: llama3_lora/checkpoint-372/ (stored 0%)\n",
      "  adding: llama3_lora/checkpoint-372/README.md (deflated 66%)\n",
      "  adding: llama3_lora/checkpoint-372/tokenizer_config.json (deflated 96%)\n",
      "  adding: llama3_lora/checkpoint-372/trainer_state.json (deflated 78%)\n",
      "  adding: llama3_lora/checkpoint-372/training_args.bin (deflated 51%)\n",
      "  adding: llama3_lora/checkpoint-372/optimizer.pt (deflated 9%)\n",
      "  adding: llama3_lora/checkpoint-372/adapter_config.json (deflated 55%)\n",
      "  adding: llama3_lora/checkpoint-372/scheduler.pt (deflated 55%)\n",
      "  adding: llama3_lora/checkpoint-372/tokenizer.json (deflated 85%)\n",
      "  adding: llama3_lora/checkpoint-372/rng_state.pth (deflated 25%)\n",
      "  adding: llama3_lora/checkpoint-372/special_tokens_map.json (deflated 76%)\n",
      "  adding: llama3_lora/checkpoint-372/scaler.pt (deflated 60%)\n",
      "  adding: llama3_lora/checkpoint-372/adapter_model.safetensors (deflated 8%)\n",
      "  adding: llama3_lora/tokenizer.json (deflated 85%)\n",
      "  adding: llama3_lora/special_tokens_map.json (deflated 76%)\n",
      "  adding: llama3_lora/train_results.json (deflated 36%)\n",
      "  adding: llama3_lora/all_results.json (deflated 36%)\n",
      "  adding: llama3_lora/adapter_model.safetensors (deflated 8%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 使用webUI进行对话"
   ],
   "metadata": {
    "id": "M8h9zQacpMin"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/LLaMA-Factory/\n",
    "!GRADIO_SHARE=1 llamafactory-cli webui"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFaPFRsgf7NZ",
    "outputId": "69427cf8-a405-4189-c421-57c38f866cb3"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/LLaMA-Factory\n",
      "2025-04-05 09:15:49.609798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743844549.630948   28820 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743844549.637203   28820 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-05 09:15:49.659867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://824ff5a234a01422f6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "[WARNING|2025-04-05 09:17:51] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:51,930 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-05 09:17:52,442 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 09:17:53,527 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 09:17:53,528 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,782 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,782 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,783 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,783 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,783 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2060] 2025-04-05 09:17:53,783 >> loading file chat_template.jinja from cache at None\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-04-05 09:17:54,261 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:699] 2025-04-05 09:17:54,552 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/config.json\n",
      "[INFO|configuration_utils.py:771] 2025-04-05 09:17:54,553 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.50.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-04-05 09:17:54] llamafactory.model.model_utils.quantization:143 >> Loading ?-bit BITSANDBYTES-quantized model.\n",
      "[INFO|2025-04-05 09:17:54] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|quantization_config.py:436] 2025-04-05 09:17:54,774 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:1154] 2025-04-05 09:17:54,840 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/model.safetensors\n",
      "[INFO|modeling_utils.py:2170] 2025-04-05 09:17:54,844 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1139] 2025-04-05 09:17:54,846 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"pad_token_id\": 128255\n",
      "}\n",
      "\n",
      "[INFO|quantizer_bnb_4bit.py:124] 2025-04-05 09:17:55,013 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "[INFO|modeling_utils.py:4987] 2025-04-05 09:18:12,094 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4995] 2025-04-05 09:18:12,095 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/llama-3-8b-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1094] 2025-04-05 09:18:12,360 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--llama-3-8b-Instruct-bnb-4bit/snapshots/fd5a4dc328319c1cfe9489eccfb9c6406bdfd469/generation_config.json\n",
      "[INFO|configuration_utils.py:1139] 2025-04-05 09:18:12,361 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 8192,\n",
      "  \"pad_token_id\": 128255,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-04-05 09:18:12] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-04-05 09:18:13] llamafactory.model.adapter:143 >> Loaded adapter(s): /content/LLaMA-Factory/llama3_lora\n",
      "[INFO|2025-04-05 09:18:13] llamafactory.model.loader:143 >> all params: 8,051,232,768\n",
      "[WARNING|2025-04-05 09:18:13] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2963, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 121, in main\n",
      "    run_web_ui()\n",
      "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2869, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2965, in block_thread\n",
      "    print(\"Keyboard interruption in main thread... closing server.\")\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\", line 146, in write_with_callbacks\n",
      "    def write_with_callbacks(s: AnyStr, /) -> int:\n",
      "\n",
      "KeyboardInterrupt\n",
      "Killing tunnel 0.0.0.0:7860 <> https://824ff5a234a01422f6.gradio.live\n"
     ]
    }
   ]
  }
 ]
}
